{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Topic Models\n",
    "\n",
    "For this exploration, I have created a toy corpus of 10 texts drawn from a larger collection of Louisiana treasure legends. I am keeping the code simple, using only the stop words and tokenization built into Sci-Kit Learn. My plan is to generate the TF needed for LDA and the TFIDF needed for NMF, to label them and save them as CSV files. Then to generate the LDA and NMD topic models, using 2 or 3 components, and to break out the H and W matrices. I hope to be able to convert those arrays to dataframes, attach the labels we need to see what's going on and save those to CSV files as well. With any luck, I can fold things into an Excel workbook, and we can be done with this.\n",
    "\n",
    "On the matter of large numbers attached to features associated with LDA components, see this on [SO](https://stackoverflow.com/questions/35140117/how-to-interpret-lda-components-using-sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is a <class 'list'> of 10 items.\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Load the corpus from a small collection of files\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "import glob\n",
    "\n",
    "file_list = glob.glob('../texts/tentexts' + '/*.txt')\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file) as f_input:\n",
    "        corpus.append(f_input.read().replace('\\n', ' '))\n",
    "        \n",
    "print(\"Corpus is a {} of {} items.\".format(type(corpus), len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ANC-088', 'ANC-089', 'ANC-090', 'ANC-091', 'LAU-013', 'LAU-014', 'LOH-157', 'LOH-158', 'LOH-159', 'LOH-160']\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Glean the filenames from the glob list\n",
    "# (We will get the feature names later from `.get_feature_names()` method.)\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "filenames = [s.replace('../texts/tentexts/', '') for s in file_list]\n",
    "docs = [s.replace('.txt', '').upper() for s in filenames]\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Parameters, imports, and functions for both LDA and NMF\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "# Import\n",
    "import numpy as np # not sure if this is still needed\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# Parameters\n",
    "n_features = 1000\n",
    "n_components = 3\n",
    "n_top_words = 10\n",
    "#stopwords = re.split('\\s+', open('../data/stopwords_all.txt', 'r').read().lower())\n",
    "\n",
    "stop_added = [\"said\", \"went\", \"know\", \"come\", \"came\", \"saw\", \"yeah\", \"like\", \"don\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(stop_added)\n",
    "# print(stop_words) # To check if words have been added\n",
    "\n",
    "# Create labels for the topics array later\n",
    "topic_labels = [\"Topic \" + str(i) for i in range(n_components)]\n",
    "\n",
    "\n",
    "# Ye olde \"let's see the topic\" function made as readable as possible\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"{:d}: \".format(topic_idx)\n",
    "        message += \" \".join([feature_names[i] + ' ' + str(round(topic[i], 2)) + ','\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: man 11.61, just 8.01, money 7.41, little 6.03, woods 5.37, told 5.28, old 4.73, later 4.69, bull 4.69, coming 4.69,\n",
      "1: man 10.06, tree 6.05, house 5.33, got 4.61, shovel 3.99, look 3.96, right 3.96, guy 3.93, money 3.93, water 3.33,\n",
      "2: gold 6.19, house 4.71, buried 4.02, lot 3.37, supposedly 3.27, money 2.74, man 2.7, little 2.66, wife 2.64, time 2.63,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# LDA\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stop_words)\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tf_array = tf.toarray()\n",
    "# np.savetxt(\"../outputs/tentexts_tf.csv\", tf_array.astype(np.int), fmt='%d', delimiter=\",\")\n",
    "# print(\"A tf array of {} has been saved to CSV.\".format(tf.shape))\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_components, \n",
    "                                max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Topic 0   Topic 1   Topic 2\n",
      "anc-088  0.987878  0.006237  0.005885\n",
      "anc-089  0.966938  0.017512  0.015550\n",
      "anc-090  0.969784  0.014724  0.015493\n",
      "anc-091  0.012094  0.974399  0.013506\n",
      "lau-013  0.988502  0.006191  0.005307\n",
      "lau-014  0.003010  0.994051  0.002939\n",
      "loh-157  0.005482  0.005508  0.989011\n",
      "loh-158  0.980826  0.009206  0.009968\n",
      "loh-159  0.009861  0.009769  0.980371\n",
      "loh-160  0.992503  0.003673  0.003824\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Create dataframes of TF, H, and W\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "# Create TF dataframe\n",
    "df_tf = pd.DataFrame(data= tf_array, index = docs, columns = tf_feature_names)\n",
    "\n",
    "# Uncomment to glimpse dataframe\n",
    "# df_tf.head(10)\n",
    "\n",
    "# Save TF dataframe to CSV file\n",
    "df_tf.to_csv('../outputs/tf_frame.csv', sep=',')\n",
    "\n",
    "# Get W (DTM) and H (WTM) arrays\n",
    "lda_W = lda.transform(tf)\n",
    "lda_H = lda.components_\n",
    "\n",
    "df_lda_DTM = pd.DataFrame(data= lda_W, index = docs, columns = topic_labels)\n",
    "df_lda_DTM.to_csv('../outputs/lda_DTM.csv', sep=',')\n",
    "print(df_lda_DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         american     asked       big    branch   brother     built      bull  \\\n",
      "Topic 0  1.959685  1.922182  2.660974  0.513983  3.277904  1.917683  4.688339   \n",
      "Topic 1  0.526864  0.615052  1.217397  1.207246  0.560899  0.544669  0.554175   \n",
      "Topic 2  0.583336  0.556125  0.585222  1.922700  1.228667  1.208030  0.574554   \n",
      "\n",
      "           buried     chain     chest    ...        water       way     weird  \\\n",
      "Topic 0  1.908744  1.238650  2.609155    ...     0.538923  2.625584  1.247875   \n",
      "Topic 1  1.166469  1.207228  0.523462    ...     3.325225  1.925063  1.275115   \n",
      "Topic 2  4.021302  0.526372  0.545059    ...     1.987316  1.987423  0.514261   \n",
      "\n",
      "             wife      wind     woods   working      yard     years     young  \n",
      "Topic 0  1.918669  1.208791  5.374326  0.540871  1.262748  2.591511  2.561572  \n",
      "Topic 1  0.597039  3.310254  0.559850  1.218500  1.908640  1.880450  0.554208  \n",
      "Topic 2  2.640340  0.553687  0.549913  1.164151  1.244924  0.602379  1.230378  \n",
      "\n",
      "[3 rows x 123 columns]\n"
     ]
    }
   ],
   "source": [
    "df_lda_WTM = pd.DataFrame(data = lda_H, index = topic_labels, columns = tf_feature_names)\n",
    "df_lda_WTM.to_csv('../outputs/lda_WTM.csv', sep=',')\n",
    "print(df_lda_WTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: man 0.36, just 0.18, house 0.18, controller 0.17, told 0.15, got 0.15, bull 0.14, old 0.13, coming 0.13, gold 0.12,\n",
      "1: wasn 0.25, place 0.24, things 0.23, water 0.23, years 0.19, supposedly 0.12, end 0.12, money 0.11, true 0.11, chain 0.1,\n",
      "2: woods 0.37, little 0.36, money 0.15, used 0.14, grave 0.13, supposed 0.13, seen 0.13, mother 0.12, longer 0.12, did 0.12,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# NMF\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words=stop_words)\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tfidf_array = tfidf.toarray()\n",
    "# np.savetxt(\"../outputs/tentexts_tfidf.csv\", tfidf_array, delimiter=\",\")\n",
    "# print(\"A tf-idf array of {} has been saved to CSV.\".format(tfidf.shape))\n",
    "\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Topic 0   Topic 1   Topic 2\n",
      "anc-088  0.705351  0.000000  0.000000\n",
      "anc-089  0.641042  0.000000  0.000000\n",
      "anc-090  0.000000  0.000000  1.193289\n",
      "anc-091  0.000000  1.255067  0.000000\n",
      "lau-013  0.549254  0.051384  0.000000\n",
      "lau-014  0.712449  0.000000  0.000000\n",
      "loh-157  0.494898  0.118592  0.000000\n",
      "loh-158  0.456827  0.000000  0.059042\n",
      "loh-159  0.215030  0.214393  0.246293\n",
      "loh-160  0.658819  0.000000  0.110136\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# TFIDF and NMF's H and W Matrices\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "tfidf_df = pd.DataFrame(data= tfidf_array, index = docs, columns = tf_feature_names)\n",
    "tfidf_df.to_csv('../outputs/tfidf_frame.csv', sep=',')\n",
    "# tfidf_df.head(10) # To see the first few columns of the tfidf array\n",
    "\n",
    "# Get W (DTM) and H (WTM) arrays\n",
    "nmf_DTM = nmf.transform(tfidf)\n",
    "nmf_WTM = nmf.components_\n",
    "\n",
    "df_nmf_DTM = pd.DataFrame(data= nmf_DTM, index = docs, columns = topic_labels)\n",
    "df_nmf_DTM.to_csv('../outputs/nmf_DTM.csv', sep=',')\n",
    "print(df_nmf_DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         american     asked       big    branch   brother     built      bull  \\\n",
      "Topic 0  0.025807  0.029743  0.061154  0.020184  0.036086  0.033742  0.135488   \n",
      "Topic 1  0.000000  0.000000  0.066460  0.008693  0.000000  0.000000  0.000000   \n",
      "Topic 2  0.000000  0.000000  0.000000  0.016023  0.111004  0.000000  0.000000   \n",
      "\n",
      "           buried     chain     chest    ...        water       way     weird  \\\n",
      "Topic 0  0.071519  0.000000  0.062882    ...     0.017865  0.045446  0.019096   \n",
      "Topic 1  0.088629  0.103527  0.000000    ...     0.231232  0.076397  0.000000   \n",
      "Topic 2  0.000000  0.000000  0.000000    ...     0.002343  0.000000  0.000000   \n",
      "\n",
      "             wife      wind     woods   working      yard    years     young  \n",
      "Topic 0  0.084598  0.094021  0.082012  0.008862  0.040743  0.03211  0.049947  \n",
      "Topic 1  0.000000  0.000000  0.000000  0.000000  0.000000  0.19483  0.000000  \n",
      "Topic 2  0.000000  0.000000  0.373551  0.000000  0.000000  0.00000  0.000000  \n",
      "\n",
      "[3 rows x 123 columns]\n"
     ]
    }
   ],
   "source": [
    "df_nmf_WTM = pd.DataFrame(data = nmf_WTM, index = topic_labels, columns = tf_feature_names)\n",
    "df_nmf_WTM.to_csv('../outputs/nmf_WTM.csv', sep=',')\n",
    "print(df_nmf_WTM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
