{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Topic Models\n",
    "\n",
    "I am creating this notebook in order to understand the numbers associated with topics in both  NMF and LDA topic models. In the case of the latter, we are seeing very large numbers associated with keywords within a given topic, and we would like to know why. But that also reveals that I don't understand what the number associated with a given word within a topic means within an NMF topic model -- is it, for example, simply the TFIDF value? Unknown.\n",
    "\n",
    "For this exploration, I have created a toy corpus of 10 texts drawn from a larger collection of Louisiana treasure legends. I am keeping the code simple, using only the stop words and tokenization built into Sci-Kit Learn. My plan is to generate the TF needed for LDA and the TFIDF needed for NMF, to label them and save them as CSV files. Then to generate the LDA and NMD topic models, using 2 or 3 components, and to break out the H and W matrices. I hope to be able to convert those arrays to dataframes, attach the labels we need to see what's going on and save those to CSV files as well. With any luck, I can fold things into an Excel workbook, and we can be done with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is a <class 'list'> of 10 items.\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Load the corpus from a small collection of files\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "import glob\n",
    "\n",
    "file_list = glob.glob('../texts/tentexts' + '/*.txt')\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file) as f_input:\n",
    "        corpus.append(f_input.read().replace('\\n', ' '))\n",
    "        \n",
    "print(\"Corpus is a {} of {} items.\".format(type(corpus), len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anc-088', 'anc-089', 'anc-090', 'anc-091', 'lau-013', 'lau-014', 'loh-157', 'loh-158', 'loh-159', 'loh-160']\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Glean the filenames from the glob list\n",
    "# (We will get the feature names later from `.get_feature_names()` method.)\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "filenames = [s.replace('../texts/tentexts/', '') for s in file_list]\n",
    "docs = [s.replace('.txt', '') for s in filenames]\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Parameters, imports, and functions for both LDA and NMF\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "# Import\n",
    "# Not sure if we still need this for this code: used to save arrays to CSVs,\n",
    "# but now we are using a dataframe to do that. (See commented out code below.)\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Parameters\n",
    "n_features = 1000\n",
    "n_components = 3\n",
    "n_top_words = 10\n",
    "#stopwords = re.split('\\s+', open('../data/stopwords_all.txt', 'r').read().lower())\n",
    "\n",
    "topic_labels = [\"Topic 0\", \"Topic 1\", \"Topic 2\"]\n",
    "# for i in range(0, n_components):\n",
    "\n",
    "\n",
    "# Ye olde \"let's see the topic\" function\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: said went money gold man came little know buried told\n",
      "Topic #1: young far time dollars went came things deep gold told\n",
      "Topic #2: said man like went don know just shovel tree house\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# LDA\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tf_array = tf.toarray()\n",
    "# np.savetxt(\"../outputs/tentexts_tf.csv\", tf_array.astype(np.int), fmt='%d', delimiter=\",\")\n",
    "# print(\"A tf array of {} has been saved to CSV.\".format(tf.shape))\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_components, \n",
    "                                max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Topic 0   Topic 1   Topic 2\n",
      "anc-088  0.406158  0.004617  0.589225\n",
      "anc-089  0.013331  0.010670  0.975999\n",
      "anc-090  0.013037  0.010633  0.976331\n",
      "anc-091  0.982436  0.008419  0.009145\n",
      "lau-013  0.004778  0.004369  0.990853\n",
      "lau-014  0.002260  0.002088  0.995652\n",
      "loh-157  0.990568  0.004557  0.004875\n",
      "loh-158  0.983257  0.007948  0.008795\n",
      "loh-159  0.983085  0.007575  0.009340\n",
      "loh-160  0.994152  0.002728  0.003120\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# Create dataframes of TF, H, and W\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "# Create TF dataframe\n",
    "df_tf = pd.DataFrame(data= tf_array, index = docs, columns = tf_feature_names)\n",
    "\n",
    "# Uncomment to glimpse dataframe\n",
    "# df_tf.head(10)\n",
    "\n",
    "# Save TF dataframe to CSV file\n",
    "df_tf.to_csv('../outputs/tf_frame.csv', sep=',')\n",
    "\n",
    "# Get W (DTM) and H (WTM) arrays\n",
    "lda_W = lda.transform(tf)\n",
    "lda_H = lda.components_\n",
    "\n",
    "df_lda_DTM = pd.DataFrame(data= lda_W, index = docs, columns = topic_labels)\n",
    "df_lda_DTM.to_csv('../outputs/lda_W.csv', sep=',')\n",
    "print(df_lda_DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         american     asked       big    branch   brother     built      bull  \\\n",
      "Topic 0  1.970122  1.595890  2.329375  1.847106  1.881802  2.609794  0.862688   \n",
      "Topic 1  0.526371  0.559859  0.512988  0.582659  0.566128  0.566889  0.543268   \n",
      "Topic 2  0.476247  0.874584  1.553584  1.231478  2.672460  0.518236  4.396376   \n",
      "\n",
      "           buried      came     chain    ...        weird      went      wife  \\\n",
      "Topic 0  6.030735  6.670249  1.231581    ...     0.511647  9.284730  3.653556   \n",
      "Topic 1  0.546082  0.609306  0.582465    ...     0.515740  0.615540  0.562227   \n",
      "Topic 2  0.589048  2.007795  1.152837    ...     1.873471  7.523704  0.931034   \n",
      "\n",
      "             wind     woods   working      yard      yeah     years     young  \n",
      "Topic 0  0.542825  0.765247  1.224481  1.231473  2.597735  3.920248  1.869225  \n",
      "Topic 1  0.548399  0.534635  0.481469  0.525065  0.527799  0.544141  0.637189  \n",
      "Topic 2  3.959836  5.125925  1.277567  2.586199  1.264871  0.575798  1.935628  \n",
      "\n",
      "[3 rows x 132 columns]\n"
     ]
    }
   ],
   "source": [
    "df_lda_WTM = pd.DataFrame(data = lda_H, index = topic_labels, columns = tf_feature_names)\n",
    "df_lda_WTM.to_csv('../outputs/lda_W.csv', sep=',')\n",
    "print(df_lda_WTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: said man went woods little like just controller know money\n",
      "Topic #1: house buried supposedly wife lot town gold lived came story\n",
      "Topic #2: yeah saw wasn water place things went years end money\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# NMF\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "tfidf_array = tfidf.toarray()\n",
    "# np.savetxt(\"../outputs/tentexts_tfidf.csv\", tfidf_array, delimiter=\",\")\n",
    "# print(\"A tf-idf array of {} has been saved to CSV.\".format(tfidf.shape))\n",
    "\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# TFIDF and NMF's H and W Matrices\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "tfidf_df = pd.DataFrame(data= tfidf_array, index = docs, columns = tf_feature_names)\n",
    "tfidf_df.to_csv('../outputs/tfidf_frame.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Topic 0   Topic 1   Topic 2\n",
      "anc-088  0.685349  0.000000  0.000000\n",
      "anc-089  0.679691  0.000000  0.021044\n",
      "anc-090  0.628755  0.000000  0.000000\n",
      "anc-091  0.019669  0.000000  1.263780\n",
      "lau-013  0.543181  0.000000  0.000000\n",
      "lau-014  0.596777  0.244314  0.000000\n",
      "loh-157  0.007589  1.245402  0.000000\n",
      "loh-158  0.348560  0.119015  0.000000\n",
      "loh-159  0.391789  0.000000  0.247836\n",
      "loh-160  0.717665  0.064456  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Get W (DTM) and H (WTM) arrays\n",
    "nmf_DTM = nmf.transform(tfidf)\n",
    "nmf_WTM = nmf.components_\n",
    "\n",
    "df_nmf_DTM = pd.DataFrame(data= nmf_DTM, index = docs, columns = topic_labels)\n",
    "df_nmf_DTM.to_csv('../outputs/nmf_DTM.csv', sep=',')\n",
    "print(df_nmf_DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         american     asked       big    branch   brother     built      bull  \\\n",
      "Topic 0   0.01775  0.024334  0.048008  0.033074  0.075008  0.010696  0.114641   \n",
      "Topic 1   0.00000  0.000000  0.000000  0.000000  0.000000  0.032530  0.000000   \n",
      "Topic 2   0.00000  0.000000  0.048711  0.011160  0.000000  0.000000  0.000000   \n",
      "\n",
      "           buried      came     chain    ...        weird      went      wife  \\\n",
      "Topic 0  0.000000  0.093302  0.000000    ...     0.012572  0.206358  0.017950   \n",
      "Topic 1  0.214652  0.129069  0.000000    ...     0.000000  0.000000  0.148566   \n",
      "Topic 2  0.050739  0.000000  0.073587    ...     0.000000  0.151543  0.000000   \n",
      "\n",
      "             wind     woods  working      yard      yeah     years     young  \n",
      "Topic 0  0.068009  0.189816  0.01299  0.014969  0.000000  0.022978  0.051542  \n",
      "Topic 1  0.000000  0.000000  0.00000  0.032976  0.000000  0.000000  0.000000  \n",
      "Topic 2  0.000000  0.000000  0.00000  0.000000  0.279275  0.148864  0.000000  \n",
      "\n",
      "[3 rows x 132 columns]\n"
     ]
    }
   ],
   "source": [
    "df_nmf_WTM = pd.DataFrame(data = nmf_WTM, index = topic_labels, columns = tf_feature_names)\n",
    "df_nmf_WTM.to_csv('../outputs/nmf_WTM.csv', sep=',')\n",
    "print(df_nmf_WTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>american</th>\n",
       "      <th>asked</th>\n",
       "      <th>big</th>\n",
       "      <th>branch</th>\n",
       "      <th>brother</th>\n",
       "      <th>built</th>\n",
       "      <th>bull</th>\n",
       "      <th>buried</th>\n",
       "      <th>came</th>\n",
       "      <th>chain</th>\n",
       "      <th>...</th>\n",
       "      <th>weird</th>\n",
       "      <th>went</th>\n",
       "      <th>wife</th>\n",
       "      <th>wind</th>\n",
       "      <th>woods</th>\n",
       "      <th>working</th>\n",
       "      <th>yard</th>\n",
       "      <th>yeah</th>\n",
       "      <th>years</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anc-088</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098376</td>\n",
       "      <td>0.076520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102743</td>\n",
       "      <td>0.196753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anc-089</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187963</td>\n",
       "      <td>0.164446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anc-090</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anc-091</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417232</td>\n",
       "      <td>0.243352</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lau-013</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104852</td>\n",
       "      <td>0.164259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lau-014</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047730</td>\n",
       "      <td>0.074773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190921</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047730</td>\n",
       "      <td>0.083517</td>\n",
       "      <td>0.047730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loh-157</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316247</td>\n",
       "      <td>0.227206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loh-158</th>\n",
       "      <td>0.140196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245310</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loh-159</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296788</td>\n",
       "      <td>0.115426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loh-160</th>\n",
       "      <td>0.071295</td>\n",
       "      <td>0.071295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055455</td>\n",
       "      <td>0.062374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055455</td>\n",
       "      <td>0.099604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062374</td>\n",
       "      <td>0.062374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         american     asked       big    branch   brother     built      bull  \\\n",
       "anc-088  0.000000  0.098376  0.076520  0.000000  0.000000  0.000000  0.295129   \n",
       "anc-089  0.000000  0.000000  0.146204  0.000000  0.000000  0.000000  0.000000   \n",
       "anc-090  0.000000  0.000000  0.000000  0.000000  0.139650  0.000000  0.000000   \n",
       "anc-091  0.000000  0.000000  0.108179  0.000000  0.000000  0.000000  0.000000   \n",
       "lau-013  0.000000  0.000000  0.000000  0.000000  0.163115  0.000000  0.314556   \n",
       "lau-014  0.000000  0.000000  0.000000  0.047730  0.000000  0.000000  0.000000   \n",
       "loh-157  0.000000  0.000000  0.000000  0.000000  0.000000  0.071141  0.000000   \n",
       "loh-158  0.140196  0.000000  0.109049  0.000000  0.000000  0.122655  0.000000   \n",
       "loh-159  0.000000  0.000000  0.000000  0.296788  0.115426  0.000000  0.000000   \n",
       "loh-160  0.071295  0.071295  0.000000  0.000000  0.055455  0.062374  0.000000   \n",
       "\n",
       "           buried      came     chain    ...        weird      went      wife  \\\n",
       "anc-088  0.000000  0.206159  0.000000    ...     0.000000  0.102743  0.196753   \n",
       "anc-089  0.000000  0.131299  0.000000    ...     0.000000  0.196306  0.000000   \n",
       "anc-090  0.000000  0.000000  0.000000    ...     0.000000  0.093754  0.000000   \n",
       "anc-091  0.108179  0.000000  0.139077    ...     0.000000  0.217876  0.000000   \n",
       "lau-013  0.000000  0.000000  0.104852    ...     0.104852  0.164259  0.000000   \n",
       "lau-014  0.000000  0.000000  0.000000    ...     0.047730  0.074773  0.000000   \n",
       "loh-157  0.316247  0.227206  0.000000    ...     0.000000  0.000000  0.243945   \n",
       "loh-158  0.109049  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "loh-159  0.000000  0.103659  0.000000    ...     0.000000  0.232471  0.000000   \n",
       "loh-160  0.055455  0.099604  0.000000    ...     0.000000  0.223377  0.000000   \n",
       "\n",
       "             wind     woods   working      yard      yeah     years     young  \n",
       "anc-088  0.000000  0.258203  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "anc-089  0.187963  0.164446  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "anc-090  0.000000  0.471223  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "anc-091  0.000000  0.000000  0.000000  0.000000  0.417232  0.243352  0.000000  \n",
       "lau-013  0.000000  0.000000  0.000000  0.091733  0.000000  0.000000  0.183466  \n",
       "lau-014  0.190921  0.000000  0.047730  0.083517  0.047730  0.000000  0.000000  \n",
       "loh-157  0.000000  0.000000  0.000000  0.071141  0.000000  0.000000  0.000000  \n",
       "loh-158  0.000000  0.000000  0.000000  0.000000  0.000000  0.245310  0.000000  \n",
       "loh-159  0.000000  0.000000  0.148394  0.000000  0.000000  0.000000  0.129827  \n",
       "loh-160  0.000000  0.000000  0.000000  0.000000  0.000000  0.062374  0.062374  \n",
       "\n",
       "[10 rows x 132 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
