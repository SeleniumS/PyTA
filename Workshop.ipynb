{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Creating a Slideshow\n",
    "\n",
    "Creating a slideshow is as easy as:\n",
    "\n",
    "    ipython nbconvert your_slides.ipynb --to slides\n",
    "\n",
    "If you want to immediately serve the slides to your web browser, just add `--post serve` at the command-line:\n",
    "\n",
    "    ipython nbconvert your_slides.ipynb --to slides --post serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Some initial magic\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "pylab.rcParams['figure.figsize'] = (12, 6)\n",
    "# from IPython.core.display import HTML\n",
    "# HTML(\"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Mining from the Command Line\n",
    "\n",
    "John Laudun  \n",
    "Department of English  \n",
    "University of Louisiana at Lafayette  \n",
    "johnlaudun@gmail.com  \n",
    "http://johnlaudun.org/  \n",
    "@johnlaudun  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "1. Getting data/texts\n",
    "2. Munging\n",
    "3. Examining\n",
    "4. Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `wget`\n",
    "\n",
    "Sometimes CLI tools, like `wget`, are more powerful than GUI tools. The key difference is that GUI tools are easier to use at first, but repetitive tasks are difficult or expensive (in terms of time). CLI tools are a little more difficult at first, but once you have an established collection of them, they are not only easier to use but just plain easier. \n",
    "\n",
    "**`wget`** is one of those tools. E.g.:\n",
    "\n",
    "    % wget -r -l 1 -w 2 --limit-rate=20k https://www.cs.cmu.edu/~spok/grimmtmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`wget` is a CLI program that retrieves web content. To my mind, since it can act like a targeted web crawler, it is the single greatest tool available to those looking to gather data/texts. \n",
    "\n",
    "Let's look at what it looks like:\n",
    "\n",
    "    % wget -r -l 1 -w 2 --limit-rate=20k https://www.cs.cmu.edu/~spok/grimmtmp/\n",
    "    \n",
    "* `-r` (or `--recursive`) turns on recursive retrieving (up to 5 directories deep). \n",
    "* `-l 1` (or`--level=1`) keeps the depth to 1.\n",
    "* `-w 2` gives the amount of time to wait between retrievals. (Two seconds lessens the server load.)\n",
    "* `--limit-rate=20k` sets the retrieval rate to 20kB/s. (This is being polite in a shared connection setting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our test case comes from Zach Isenhower: http://digital.library.okstate.edu/kappler/Vol2/Toc.htm. \n",
    "\n",
    "As it turns out, almost all the documents in which we are interested are housed in a single directory (below), which does not like being crawled. Running `wget` returns **ERROR 403: Forbidden**. In all likelihood, this is the result of the site's administrator configuring the website to make sure that directories cannot be browsed directly.\n",
    "\n",
    "    !wget -r -l 1 -w 2 --limit-rate=20k http://digital.library.okstate.edu/kappler/Vol2/treaties/\n",
    "\n",
    "We need, then, to be able to access the table of contents above, get all the links listed, and then download that list into a directory (folder in GUI terms) of our choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# To use this script, the user needs to provide the three values below: \n",
    "# myurl, myfilter, mydirectory\n",
    "# Please make sure `mydirectory` is already created before running\n",
    "\n",
    "myurl = \"http://digital.library.okstate.edu/kappler/Vol2/Toc.htm\"\n",
    "myfilter = \"http://digital.library.okstate.edu/kappler/Vol2/treaties/\"\n",
    "mydirectory = \"/Users/jjl/Desktop/downloadedfiles/\"\n",
    "\n",
    "myconnection = urllib.request.urlopen(myurl)\n",
    "myhtml = myconnection.read()\n",
    "mysoup = BeautifulSoup(myhtml, \"lxml\")\n",
    "mylinks = mysoup.find_all('a')\n",
    "\n",
    "all_links = []\n",
    "for tag in mylinks:\n",
    "    link = tag.get('href',None)\n",
    "    if link is not None:\n",
    "        all_links.append(link)\n",
    "\n",
    "myresults = [k for k in all_links if myfilter in k]\n",
    "\n",
    "for result in myresults:\n",
    "    remotefile = urllib.request.urlopen(result)\n",
    "    localfile = open(mydirectory+result.replace(myfilter, ''),'wb')\n",
    "    localfile.write(remotefile.read())\n",
    "    localfile.close()\n",
    "    remotefile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To work with this script, all you need to do is provide three variables:\n",
    "\n",
    "**myurl** is the page that lists the files\n",
    "\n",
    "**myfilter** is the (remote) directory in which the files are stored\n",
    "\n",
    "**mydirectory** is the (local) directory to which you wish to save the files. N.B.: You must create this directory ahead of time; and you must provide the absolute path from the root of your OS. The example path is from a Mac; Windows users will need something like `C:\\Documents and Setting\\username\\path\\to\\folder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we have a directory (folder) sitting on our desktop and it has all the files we want:\n",
    "\n",
    "![Screenshot of Full Directory](./Screenshot_directory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Munging\n",
    "\n",
    "> **Data munging** or **data wrangling** is loosely the process of manually converting or mapping data from one \"raw\" form into another format that allows for more convenient consumption of the data with the help of semi-automated tools. This may include further munging, data visualization, data aggregation, training a statistical model, as well as many other potential uses. Data munging as a process typically follows a set of general steps which begin with extracting the data in a raw form from the data source, \"munging\" the raw data using algorithms (e.g. sorting) or parsing the data into predefined data structures, and finally depositing the resulting content into a data sink for storage and future use.\n",
    "\n",
    "-- [Wikipedia](https://en.wikipedia.org/wiki/Data_wrangling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a look at one of Zach's files:\n",
    "\n",
    "!less apa0598.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No matter what Zach has in mind for this data, we can be pretty sure that it does not include a lot of angle brackets and funkiness like `div class=\"SANSLINE\"`. (For the record, *funkiness* is a technical term in data munging. I'm serious. Go look it up.) Whatever Zach's next steps are, he is going to want to clean up the text. \n",
    "\n",
    "For this workshop, we are going to skip transforming this html into some kind of operable xml and focus on simply getting it into useful plain text. From there, Zach will be able to engage a number of automated processes which may be more, or less, interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "myfile = open('apa0598.htm', 'r')\n",
    "myhtml = myfile.read()\n",
    "mytext = BeautifulSoup(myhtml).text\n",
    "\n",
    "print(mytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we just need to clean up the entire folder!\n",
    "\n",
    "Again, a bit of automation goes a long way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "filesIN = \"/Users/jjl/Desktop/filesIN/\"\n",
    "filesOUT = \"/Users/jjl/Desktop/filesOUT/\"\n",
    "\n",
    "postlist = os.listdir(filesIN)\n",
    "\n",
    "for post in postlist: \n",
    "    text = BeautifulSoup(open(filesIN+post), \"lxml\")\n",
    "    text.encode(\"utf-8\")\n",
    "    fout = open(filesOUT+post, \"w\")\n",
    "    fout.write(text.encode(\"utf-8\"))\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Mining with the NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Text mining** or **text analytics** (or those same phrases with *data* substituted for *text*) means a lot of things. It can mean simple textual statistics, building a concordance (or seeing key words in context), determining what words co-locate (collocate) with other words within a text or across texts, as well as what words co-occur with others across a number of texts (e.g., topic modeling).\n",
    "\n",
    "That is, **text mining** is scalable both in terms of *scope*, focusing on small or large patterns, and in terms of *range*, focusing on a single text or dozens, hundreds, thousands of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our next series of explorations, we are going to use Python's NLTK module to work at the smaller end of the scope:\n",
    "\n",
    "* create a corpus and view concordances, synonyms, collactions\n",
    "* focus on only a single text to see what we can learn about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Corpus Views\n",
    "\n",
    "A lot of our text mining can be done by treating all the texts as one large text file: collocations, concordances, and other textual features can all be explored within a global context. So, one of the common things that needs to get done is to create one text file with which to work.\n",
    "\n",
    "The way to do this in Python is to read a folder full of texts into a single and then to join those list items into a single text. This seems weird, but this particular way of doing things actually uses less processing power and memory. The good news is that compiling documents into a single list is also the form in which Python's topic modeling module, `gensim`, expects them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "file_list = glob.glob('./texts' + '/*.txt')\n",
    "\n",
    "mytexts = []\n",
    "for filename in file_list:\n",
    "    with open(filename, 'r') as f:\n",
    "        mytexts.append(f.read().replace('\\n', ' '))\n",
    "\n",
    "alltexts = ''.join(mytexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With our text now *one big text file*, we can mumble the magic incantations which lets us play with our text within the NLTK:\n",
    "\n",
    "(If you are ever curious, type `one big text file` into a search engine and saunter through the results.)\n",
    "\n",
    "**N.B.**: If you ever get the response that you are missing something from NLTK, simply run `nltk.download()` and download what you need using the GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "tokens = nltk.word_tokenize(alltexts)\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With that done, we can now begin to explore our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "myword = text.concordance(\"recommend\")\n",
    "print(myword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text.similar(\"recommend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text.common_contexts([\"husband\", \"wife\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Brief Excursus on Control Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Control logic** is what makes automation of processes possible. That is all. \n",
    "\n",
    "**Control logic** is entirely about you making the decisions and then writing those decisions down and then telling the script *Go do that while I grab a cup of coffee.*\n",
    "\n",
    "**Control logic** is nothing to be scared of. \n",
    "\n",
    "    ... Nothing of which to be scared? \n",
    "    ... Nothing that should scare you? \n",
    "    ----------------------------------------------------------------\n",
    "    LogicError                     Traceback (most recent call last)\n",
    "    <English-input-13-4c23f72ea457> in <module>(prepositions)\n",
    "    ----> syntax.prepositions()\n",
    "    \n",
    "If you can master prepositions, you can master **control logic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look through most of the scripting in this notebook, you will note that one kind of control logic dominates: the **for** loop. Its logic looks like this:\n",
    "\n",
    "```\n",
    "for thing in collection:\n",
    "    do this\n",
    "```\n",
    "\n",
    "Implicit in this command is that the `for` loop is going to keep doing `this` until it runs out of `things`, at which point it stops. No more control logic is required of us as writers. In most cases, the collection is a folder (or directory) of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's build a script that gets a list of word counts in a text, saves the counts to a CSV file. We will edit that CSV file to have a header in it, and then we will use the file to create a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "file_list = glob.glob('./texts/*.txt')\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You are going to see **`glob`** a lot in these scripts. There are a number of ways to get files in Python. I tend to use `glob` because I tend to keep the files with which I am working in a single directory. \n",
    "\n",
    "Our first task is to feed `for` the `file_list`. \n",
    "\n",
    "Remember, our logic is `for` **`thing`** `in` **`collection`**. \n",
    "\n",
    "In this instance **`thing`** is the item with which we want to do something within the `for` loop, and we get to choose the name. (It could be `thing`, if you like, but remember that choosing meaningful names is part of writing good code.) In this next bit of code, you'll note that I've chosen the names `fpath` and `f` for the two iterators. I like those two names because they are very straightforward: the former is short for *filepath* which is what the `glob` module is in fact compiling into a list for me and `f` is shorthand for *file* which is what I want regex to work on. They are also traditional in Python coding. (Remember, I'm a folklorist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for fpath in glob.glob(\"../assets/*.txt\"):\n",
    "    with open(fpath) as f:\n",
    "         fixed_text = re.sub(\"[^a-zA-Z']\", \" \" , f.read().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "At this point, we haven't really told the `for` loop what to do with all this information. \n",
    "\n",
    "For now, to keep things simple, we are going to build a **dictionary**. \n",
    "\n",
    "In Python, a dictionary is simply a container for pairs of items, where one item is the key and the other the value. (When you look up a term in the dictionary, the term is the key that allows you to access the value you seek, the definition.) \n",
    "\n",
    "For this dictionary, we are going to use the filepath as the key and we are going to pair it with a pair of numbers. (The pair of numbers is in a particular data structure known as a tuple, but let's leave the larger discussion of data structures for another time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = {}\n",
    "\n",
    "for fpath in glob.glob(\"./texts/*.txt\"):\n",
    "    with open(fpath, encoding=\"utf-9\") as f:\n",
    "         fixed_text = re.sub(\"[^a-zA-Z']\", \" \" , f.read().lower())\n",
    "    files[fpath] = [len(fixed_text.split()),len(set(fixed_text.split()))]\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Having compiled our texts and data points into a dictionary, we are close to done. We could, if we wanted to, simply run this script and grab the data from our console, or we could save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"textStats.csv\", \"w\") as fileOut:\n",
    "    for fname in files:\n",
    "        print(\"{},{},{}\".format(fname, files[fname][0], files[fname][1]), file=fileOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This time we have placed a `for` loop within a `with` statement. The `for` loop iterates over our dictionary, `files`, printing first the file name, which is the key, and then using the standard way of getting values out of a dictionary: dictionary[key]. Because our value is a list and we want one value then the other, and not both, we use a list index, 0 for the first value and 1 for the second value, to place them in the proper sequence as we build each line. \n",
    "\n",
    "With this final step, we have a CSV file that we can import into a spreadsheet application for further analysis and/or visualization. \n",
    "\n",
    "Here's the completed script, commented to make it easy to port elsewhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "\"\"\"textstats.py: basic statistics for a collection of text files\"\"\"\n",
    "\n",
    "import glob, re # import the two modules needed\n",
    "\n",
    "files = {} # create an empty dictionary which we will populate with the for loop\n",
    "\n",
    "# this for loop navigates to a directory, opens each file, normalizes the text\n",
    "# and then counts words, saving the counts to a list \n",
    "for fpath in glob.glob(\"./texts/*.txt\"):\n",
    "    with open(fpath, encoding=\"utf-8\") as f:\n",
    "         fixed_text = re.sub(\"[^a-zA-Z']\", \" \" , f.read().lower())\n",
    "    files[fpath] = [len(fixed_text.split()),len(set(fixed_text.split()))]\n",
    "\n",
    "# save the results to a file\n",
    "with open(\"textStats.csv\", \"w\") as fileOut:\n",
    "    for fname in files:\n",
    "        print(\"{},{},{}\".format(fname, files[fname][0], files[fname][1]), file=fileOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's Graph That"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"textStats.csv\", \",\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's a very tidy setup with very little work on our part. One line of code. Sorting by the longest text is just as easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "by_longest = df.sort_values(by=\"Count\", axis=0, ascending=False) \n",
    "print(by_longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "ax = by_longest[['Count','Lexicon']].plot(kind='bar', \n",
    "                                           title =\"Text Stats\",\n",
    "                                           figsize=(20,10),\n",
    "                                           legend=True)\n",
    "ax.set_xlabel(\"Text\")\n",
    "ax.set_ylabel(\"Word Count\")\n",
    "ax.set_xticklabels(list(by_longest['Text'])) \n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "\"\"\"pytaChart.py: a bar chart from a dataframe of text statistics\"\"\"\n",
    "\n",
    "import pandas as pd, matplotlib as mpl\n",
    "\n",
    "# Import data from file into dataframe\n",
    "df = pd.read_csv(\"textStats.csv\", \",\")\n",
    "\n",
    "# Sort by the longest text\n",
    "by_longest = df.sort_values(by=\"Count\", axis=0, ascending=False) \n",
    "\n",
    "# Draw the graph (note use of ggplot)\n",
    "mpl.style.use('ggplot')\n",
    "ax = by_longest[['Count','Lexicon']].plot(kind='bar', \n",
    "                                           title =\"Text Stats\",\n",
    "                                           figsize=(15,10),\n",
    "                                           legend=True,\n",
    "                                           fontsize=12)\n",
    "ax.set_xlabel(\"Text\",fontsize=12)\n",
    "ax.set_ylabel(\"Word Count\",fontsize=14)\n",
    "ax.set_xticklabels(list(by_longest['Text'])) \n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Single Text Views\n",
    "\n",
    "We can also work with single texts to understand them in new ways. Here we will work with a short story, \"The Most Dangerous Game,\" published in time for it to be both in the public domain and yet remain a part of our collective literary/semiotic tradition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# First, let's load the text file:\n",
    "mdg = open(\"./mdg.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Next, let's find out how long it is:\n",
    "len(mdg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 44078 what? Characters. We prefer words:\n",
    "\n",
    "mdgtokens = nltk.word_tokenize(mdg)\n",
    "len(mdgtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 9717 words AND punctuation marks. \n",
    "\n",
    "import re\n",
    "\n",
    "mdg_words = re.sub(\"[^a-zA-Z'-]\",\" \", open(\"./mdg.txt\").read())\n",
    "print(mdg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "len(mdg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mdg_word_list = mdg_words.split()\n",
    "print(mdg_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "len(mdg_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sorted(mdg_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sorted(set(mdg_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "len(sorted(set(mdg_word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "mdg2 = open(\"./mdg.txt\").read()\n",
    "mdg2_words = re.sub(\"[^a-zA-Z'-]\",\" \", open(\"./mdg.txt\").read().lower())\n",
    "mdg2_word_list = mdg2_words.split()\n",
    "len(sorted(set(mdg2_word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Lexical Diversity of MDG:\n",
    "len(mdg2_word_list) / len(set(mdg2_word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also use the NLTK to explore a single text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "\n",
    "mdg_raw = open(\"./mdg.txt\").read()\n",
    "mdg_words = re.sub(\"[^a-zA-Z'-]\",\" \", mdg_raw)\n",
    "mdg_case = mdg_words.lower()\n",
    "\n",
    "# print(mdg_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mdg_tokens = nltk.word_tokenize(mdg_case)\n",
    "mdg_text = nltk.Text(mdg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "len(mdg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "len(mdg_tokens) / len(set(mdg_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On average, a word occurs four times in \"The Most Dangerous Game.\"\n",
    "\n",
    "Out of curiosity, how many words occur four times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wordfrequency = nltk.FreqDist(mdg_tokens)\n",
    "four_times = [word for word in wordfrequency.keys() if wordfrequency[word] == 4]\n",
    "print(four_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mdg_text.count(\"dangerous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mdg_text.concordance(\"dangerous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Where does \"dangerous\" occur within the larger text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mdg_text.dispersion_plot([\"dangerous\", \"danger\", \"game\", \"fear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wordfrequency.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "freq_dic = {}\n",
    "for word in mdg_tokens:\n",
    "#        word = punctuation.sub(\"\", word)\n",
    "        try:\n",
    "            freq_dic[word] += 1\n",
    "        except: \n",
    "            freq_dic[word] = 1\n",
    "word_list = [(val, key) for key, val in freq_dic.items()]\n",
    "\n",
    "# print(sorted(word_list, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# More useful to save this to a file:\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('output.csv', 'w') as f:\n",
    "        wtr = csv.writer(f)\n",
    "        wtr.writerows(sorted(word_list, reverse=True))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Mining from Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All text mining is simply an attempt to find numerical representations of texts that are meaningful/useful to the analyst. Nothing more. If there is magic in it, then it is in the **speed** and with which it can be done and the **scale** on which it can be come. \n",
    "\n",
    "There are a lot of possibilities, but the most popular one at present is known as **topic modeling**. The mechanics of topic modeling are so instrumental in the present moment that one form of it, **LSA** (Latent Semantic Analysis) is built right into Mac OS X.\n",
    "\n",
    "That noted the topic modeling that most humanists are doing is with Mallet, which uses a significant statistical tweak: **LDA** (Latent Dirichlet Allocation). \n",
    "\n",
    "No matter what form of topic modeling is in play, the process groups words by how often they occur with other words within a text, and, then, sorts the documents involved by these word groups. (The word groupings are called topics.)\n",
    "\n",
    "**Nota bene**: Topic models, like other statistical approaches to texts, do not pay any attention to syntax: texts are reduced to \"bags of words\": it is not unknown to re-create texts as `bow`s from word frequency distributions. (Topic models don't care.)\n",
    "\n",
    "*TL;DR: (1) The math works. (2) Cognitive studies suggest it captures some essential elements of how we think about texts. (3) Make sure you choose the math that is right for your work.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Pérez, Fernando and Brian E. Granger. 2007. IPython: A System for Interactive Scientific Computing. _Computing in Science and Engineering_ 9(3): 21-29. doi:10.1109/MCSE.2007.53. URL: http://ipython.org"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
